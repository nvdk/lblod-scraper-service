import datetime
import os
import uuid
from string import Template

from itemadapter import ItemAdapter
from constants import DEFAULT_GRAPH

from escape_helpers import sparql_escape_uri
from sudo_query import update_sudo
from helpers import logger

from .file import construct_insert_file_query, STORAGE_PATH
from constants import DEFAULT_GRAPH, RESOURCE_BASE, TASK_STATUSES
from .job import update_task_status, add_stats_to_task
from .harvester import collection_has_collected_files, create_results_container

class Pipeline:
    timestamp = datetime.datetime.now()

    def __init__(self):
        self.storage_path = os.path.join(STORAGE_PATH, self.timestamp.isoformat())
        if not os.path.exists(self.storage_path):
            os.mkdir(self.storage_path)

    def open_spider(self, spider):
        pass

    def close_spider(self, spider):
        try:
            if collection_has_collected_files(spider.collection):
                create_results_container(spider.task, spider.collection)
                update_task_status(spider.task, TASK_STATUSES["SUCCESS"])
            else:
                logger.error("spider closed without collecting files")
                update_task_status(spider.task, TASK_STATUSES["FAILED"])
                stats = spider.crawler.stats.get_stats()
                if stats.get('log_count/ERROR') == 0:
                    items = stats.get("item_scraped_count", 0)
                    start_time = stats.get('start_time')
                    end_time = stats.get('end_time')
                    pages = stats.get("response_received_count", 0)
                    depth = stats.get("request_depth_max", 0)
                    add_stats_to_collection(spider.task, {
                        "start_time": start_time,
                        "end_time": end_time,
                        "pages": pages,
                        "items": items,
                        "depth": depth
                    })
        except Exception as e:
            logger.error(e)
            logger.error("failure while closing spider, attempting to set task to failed")
            update_task_status(spider.task, TASK_STATUSES["FAILED"])


    def process_spider_exception(self, response, exception, spider):
        # Extract the relevant information from the failed response
        url = response.url
        status = response.status
        error_message = str(exception)
        logger.error(error_message)

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        contents = adapter.get("contents")
        remote_data_object = adapter.get("rdo")
        if isinstance(contents, (bytes, bytearray)):
            write_mode = "wb"
        elif isinstance(contents, str):
            write_mode = "w"
        else:
            # Can't write a file that isn't a (byte)string
            return item

        _uuid = str(uuid.uuid4())
        physical_file_name = f"{_uuid}.html"
        physical_file_path = os.path.join(self.storage_path, physical_file_name)
        with open(physical_file_path, write_mode) as f:
            f.write(contents)
            f.seek(0, os.SEEK_END) # TODO: check if this can be replaced by return value of write
            size = f.tell()
            file_created = datetime.datetime.now()

        adapter["uuid"] = _uuid
        adapter["size"] = size
        adapter["file_created"] = file_created
        adapter["extension"] = "html"
        adapter["format"] = "text/html; charset=utf-8"
        adapter["physical_file_name"] = physical_file_name
        adapter["physical_file_path"] = physical_file_path
        try:
            self.push_item_to_triplestore(adapter)
        except Exception as e:
            logger.error(f"Encountered exception while trying to write data to triplestore for item generated by scraping {adapter['url']}")
            update_task_status(spider.task, TASK_STATUSES["FAILED"])
            raise e from None

        return item

    def push_item_to_triplestore(self, item):
        virtual_resource_uuid = str(uuid.uuid4())
        virtual_resource_uri = f"http://data.lblod.info/files/{virtual_resource_uuid}"
        virtual_resource_name = f"{virtual_resource_uuid}.{item['extension']}"
        file = {
            "uri": virtual_resource_uri,
            "uuid": virtual_resource_uuid,
            "name": virtual_resource_name,
            "mimetype": item["format"],
            "created": item["file_created"],
            "modified": item["file_created"], # currently unused
            "size": item["size"],
            "extension": item["extension"],
            "remote_data_object": item["rdo"]["uri"]
        }
        physical_resource_uri = item["physical_file_path"].replace("/share/", "share://") # TODO: use file lib function to construct share uri
        physical_file = {
            "uuid": item["uuid"],
            "uri": physical_resource_uri,
            "name": item["physical_file_name"]
        }

        ins_file_q_string = construct_insert_file_query(file,
                                                        physical_file,
                                                        DEFAULT_GRAPH)
        update_sudo(ins_file_q_string)
